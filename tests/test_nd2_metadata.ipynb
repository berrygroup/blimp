{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nd2reader import ND2Reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_acquisition_metadata_planes(l):\n",
    "    # https://stackoverflow.com/questions/69832116/split-a-list-into-sublists-based-on-the-value-of-an-element\n",
    "    x = [i for i, s in enumerate(l) if re.search(\"^Plane\",s.lstrip())]\n",
    "    y = x[1:] + [len(l)]\n",
    "    z = [l[i:j] for i, j in zip(x, y)]\n",
    "    return(z)\n",
    "\n",
    "\n",
    "def parse_additional_metadata(acq_metadata):\n",
    "    metadata_planes = split_acquisition_metadata_planes(\n",
    "        acq_metadata['TextInfoItem_5'].split('\\r\\n')\n",
    "    )\n",
    "    metadata = ['\\\\n'.join(plane) for plane in metadata_planes]\n",
    "    metadata = [s.replace(',',';') for s in metadata]\n",
    "    return(metadata)\n",
    "\n",
    "\n",
    "def get_start_time_abs(metadata_dict,common_metadata):\n",
    "    start_time_abs = metadata_dict['date']\n",
    "\n",
    "    if (start_time_abs is None):\n",
    "        start_time_abs = common_metadata['TextInfoItem_9']\n",
    "        start_time_abs = datetime.datetime.strptime(start_time_abs, '%d/%m/%Y  %I:%M:%S %p')\n",
    "    return(start_time_abs)\n",
    "    \n",
    "\n",
    "def get_standard_field_id_mapping(df):\n",
    "    image_metadata = df\n",
    "    image_metadata = image_metadata[['field_id','stage_x_abs','stage_y_abs']].groupby('field_id').mean()\n",
    "    image_metadata[['stage_x_abs','stage_y_abs']] = image_metadata[['stage_x_abs','stage_y_abs']].round()\n",
    "    image_metadata['XYCoordinates']= image_metadata[['stage_x_abs','stage_y_abs']].apply(tuple, axis=1)\n",
    "    image_metadata = image_metadata.reset_index()\n",
    "\n",
    "    # Number fields from top-left to bottom-right (increase x first)\n",
    "    unique_int_coords_sorted = sorted(list(set(image_metadata['XYCoordinates'])) , key=lambda k: [-k[1], k[0]])\n",
    "    coord_index = dict(zip(unique_int_coords_sorted, [\"%0d\" %i for i in range(1,len(unique_int_coords_sorted)+1)])) \n",
    "\n",
    "    # keep this as StandardFieldID\n",
    "    image_metadata['standard_field_id'] = image_metadata['XYCoordinates'].map(coord_index)\n",
    "    return(image_metadata[['field_id','standard_field_id']])\n",
    "    \n",
    "    \n",
    "def extract_metadata_and_save(\n",
    "    in_file_path : Union[str,Path],\n",
    "    out_path : Union[str,Path]):\n",
    "        \n",
    "    nd2_file = ND2Reader(in_file_path)\n",
    "    acquisition_times = [t for t in nd2_file.parser._raw_metadata.acquisition_times]\n",
    "    common_metadata = nd2_file.parser._raw_metadata.image_text_info[b'SLxImageTextInfo']\n",
    "    common_metadata = { key.decode(): val.decode() for key, val in common_metadata.items() }\n",
    "\n",
    "    # save 'SLxImageTextInfo' as JSON\n",
    "    json_file_path = Path(out_path) / Path(Path(in_file_path).stem +'.json')\n",
    "    with open(json_file_path, \"w\") as outfile:\n",
    "        json.dump(common_metadata, outfile)\n",
    "\n",
    "    # parse metadata\n",
    "    metadata_dict = nd2_file.parser._raw_metadata.__dict__\n",
    "    additional_metadata = parse_additional_metadata(common_metadata)\n",
    "    additional_metadata_df = pd.DataFrame(additional_metadata).T\n",
    "    additional_metadata_df.columns = ['metadata_string_acquisition_' + str(i) for i in range(0,len(additional_metadata))]\n",
    "    \n",
    "    # combine into dataframe\n",
    "    metadata_df = pd.DataFrame(\n",
    "        data={\n",
    "            'n_pixels_y' : metadata_dict['height'],\n",
    "            'n_pixels_x' : metadata_dict['width'],\n",
    "            'objective_name' : common_metadata['TextInfoItem_13'],\n",
    "            'pixel_size_microns' : metadata_dict['pixel_microns'],\n",
    "            'stage_x_abs' : nd2_file.parser._raw_metadata.x_data,\n",
    "            'stage_y_abs' : nd2_file.parser._raw_metadata.y_data,\n",
    "            'stage_z_abs' : nd2_file.parser._raw_metadata.z_data,\n",
    "            'acquisition_time_rel' : acquisition_times,\n",
    "            'stage_z_id' : list(metadata_dict['z_levels'])*(nd2_file.sizes['t']*nd2_file.sizes['v']),\n",
    "            'field_id' : list(np.repeat(range(1,1 + nd2_file.sizes['v']),nd2_file.sizes['z']))*nd2_file.sizes['t']})\n",
    "    \n",
    "    metadata_df['filename_ome_tiff'] = [Path(in_file_path).stem + '_' + str(f).zfill(4) + '.ome.tiff' for f in metadata_df['field_id']]\n",
    "    \n",
    "    start_time_abs = get_start_time_abs(metadata_dict,common_metadata)\n",
    "    if (start_time_abs is not None):\n",
    "        metadata_df['acquisition_time_abs']=[start_time_abs + datetime.timedelta(seconds=x) for x in metadata_df['acquisition_time_rel']]\n",
    "\n",
    "    # standardise field id (top-left to bottom-right)\n",
    "    standard_field_id_mapping = get_standard_field_id_mapping(metadata_df)\n",
    "    metadata_df = pd.merge(metadata_df, standard_field_id_mapping,on = 'field_id', how = 'left')\n",
    "    \n",
    "    # add additional metadata as columns\n",
    "    metadata_df = pd.merge(metadata_df, additional_metadata_df,how='cross')\n",
    "    \n",
    "    # write metadata to file\n",
    "    with Path(out_path) / Path(Path(in_file_path).stem +'_metadata.csv') as out_file_path:\n",
    "        metadata_df.to_csv(out_file_path, index=False)\n",
    "    with Path(out_path) / Path(Path(in_file_path).stem +'_metadata.pkl') as out_file_path:\n",
    "        metadata_df.to_pickle(out_file_path)\n",
    "    \n",
    "    return(metadata_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file_path = '/srv/scratch/berrylab/z3532965/Nikon_AX_QPI/20221010_BleachChase_POLR2A/20221010_144534_824/WellE05_ChannelGFP,AF647_Seq0001.nd2'\n",
    "#in_file_path = '/srv/scratch/berrylab/z3536241/NikonNSTORM/221216/221212_Pbody_DDXImmuno/20221216_140317_611/Well01_ChannelAG_647_FISH,AG_568_FISH,AG_488NHS,AG_DAPI_Seq0000.nd2'\n",
    "out_path = '/srv/scratch/berrylab/z3532965/Nikon_AX_QPI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = extract_metadata_and_save(in_file_path,out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "60b8c657399aad6f9d89cbf9465e7c1c57998666dc051fd8b62b971f18a5d01e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
