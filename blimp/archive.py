"""Convert Nikon nd2 files to standard open microscopy environment formats."""
from typing import Dict, List, Union
from pathlib import Path
from datetime import datetime
import os
import re
import glob
import stat
import logging

from blimp.preprocessing.convert_nd2 import find_nd2_files
from blimp.preprocessing.convert_operetta import find_images_dirs

logger = logging.getLogger(__name__)


def check_config_file() -> bool:
    """Check that the config file exists in the correct location"""
    config_path = Path.home() / "config.cfg"
    if not config_path.exists():
        logger.error(
            f"Config file not found at {config_path}. Please use get-config-file from UNSW's data archive module"
        )
        os._exit(1)
    else:
        return True


def write_archiving_script_nd2(
    file_paths: Union[List[Path], List[str]],
    script_path: Union[Path, str],
    first_name: str,
    project_name: str = "D0419427",
) -> None:
    """
    Create a bash script that execute the 'upload.sh' command for each file path in `file_paths` list.
    The script will be saved in the current working directory with name 'upload_script.sh'

    Parameters
    ----------
    file_paths : list of str
        list of file paths to be uploaded

    Returns
    -------
    None
    """

    file_paths = [str(f) for f in file_paths]

    with open(Path(script_path), "w") as f:
        f.write("#!/bin/bash\n\n")
        f.write("## Archiving script generated by BLIMP's archive tool\n\n")
        f.write("## Inputs:\n")
        f.write(f"##     First name: {first_name}\n")
        f.write(f"##     Project:    {project_name}\n\n")
        f.write("module add unswdataarchive/2021-02-17\n\n")
        f.write("export CONFIG_FILE=${HOME}/config.cfg\n\n")
        for file_path in file_paths:
            # use a regex to strip the first part of the filename that
            # we do not need to include in the upload destination
            relative_path = re.sub(r"^\/srv\/scratch\/berrylab\/z\d{7}\/", "", file_path)
            f.write(
                f"java -Dmf.cfg=$CONFIG_FILE -cp /apps/unswdataarchive/2021-02-17/aterm.jar arc.mf.command.Execute import -verbose true -import-empty-folders true -namespace /UNSW_RDS/{project_name}/{first_name}/{str(Path(relative_path).parent)} '{file_path}'\n"
            )


def split_operetta_files_into_archiving_batches(images_dir: Union[Path, str]) -> Dict[str, List[str]]:
    """Splits filenames into groups based on the 'r00c00' pattern.

    Parameters
    ----------
        images_dir: string or pathlib.Path of the 'Images' directory.

    Returns
    -------
        A dictionary containing groups of filenames as values,
        with the 'r00c00' pattern as keys.
    """

    filenames = os.listdir(images_dir)
    groups: Dict[str, List[str]] = {}

    pattern = r"r(\d{2})c(\d{2})"  # Regular expression pattern for 'r00c00'
    for filename in filenames:
        match = re.search(pattern, filename)
        if match:
            key = match.group()  # Get the matched pattern
            if key not in groups:
                groups[key] = []
            groups[key].append(filename)

    return groups


def write_archiving_batch_files(
    archive_dir: Union[Path, str], images_dir: Union[Path, str], groups: Dict[str, List[str]]
) -> None:
    """Writes groups of filenames to separate text files,
    for use as input to `tar` with the '-T' option.

    Parameters
    ----------
        groups: A dictionary containing groups of filenames,
        where keys are 'r00c00' patterns and values are lists
        of filenames.
    """
    archive_dir = Path(archive_dir)
    images_dir = Path(images_dir)
    for key, image_filenames in groups.items():
        archive_batch_filename = archive_dir / (key + ".txt")
        try:
            logger.debug(f"Writing batch file {archive_batch_filename}")
            with open(archive_batch_filename, "w") as file:
                for image_filename in image_filenames:
                    file.write((str(images_dir / image_filename)) + "\n")
        except OSError:
            raise FileNotFoundError(f"Cannot write to {archive_batch_filename}.")
    return


def write_archiving_script_operetta(
    file_paths: Union[List[Path], List[str]],
    script_path: Union[Path, str],
    first_name: str,
    project_name: str = "D0419427",
    append: bool = False,
) -> None:
    """
    Create a bash script that execute the 'upload.sh' command for each file path in `file_paths` list.
    The script will be saved in the current working directory with name 'upload_script.sh'

    Parameters
    ----------
    file_paths : list of str
        list of file paths to be uploaded

    Returns
    -------
    None
    """

    file_mode = "a" if append else "w"
    file_paths = [str(f) for f in file_paths]
    try:
        with open(Path(script_path), file_mode) as f:
            if not append:
                f.write("#!/bin/bash\n\n")
                f.write("## Archiving script generated by BLIMP's archive tool\n\n")
                f.write("## Inputs:\n")
                f.write(f"##     First name: {first_name}\n")
                f.write(f"##     Project:    {project_name}\n\n")
                f.write("module add unswdataarchive/2021-02-17\n\n")
                f.write("export CONFIG_FILE=${HOME}/config.cfg")
            for file_path in file_paths:
                # Identify Archive directory by name and write compression commands
                if Path(file_path).stem == "Archive":
                    archive_path = Path(file_path)
                    f.write(f"\n\n## Archiving batches from {archive_path}\n\n")
                    f.write("## Compress batches:\n\n")
                    archive_batch_files = glob.glob(str(Path(file_path) / "*.txt"))
                    for batch_file in archive_batch_files:
                        f.write(f"tar -cvzf {str(Path(batch_file).with_suffix('.tar.gz'))} -T {batch_file}\n")

            f.write("\n## Upload:\n\n")
            for file_path in file_paths:
                # use a regex to strip the first part of the filename that
                # we do not need to include in the upload destination
                relative_path = re.sub(r"^\/srv\/scratch\/berrylab\/z\d{7}\/", "", file_path)
                f.write(
                    f"java -Dmf.cfg=$CONFIG_FILE -cp /apps/unswdataarchive/2021-02-17/aterm.jar arc.mf.command.Execute import -verbose true -import-empty-folders true -namespace /UNSW_RDS/{project_name}/{first_name}/{relative_path} {file_path}\n"
                )
            if archive_path is not None:
                archive_relative_path = re.sub(r"^\/srv\/scratch\/berrylab\/z\d{7}\/", "", str(archive_path))
                local_checksum_path = archive_path.parent / "checksum_Archive_Local.csv"
                diff_checksum_path = archive_path.parent / "diff_checksum_Archive_Local_vs_UNSW_RDS.csv"
                # compute local checksums
                f.write("\n## Compute checksums locally:\n\n")
                f.write(f'csv_file="{str(local_checksum_path)}"\n')
                f.write(f'folder_path="{str(archive_path)}"\n')
                f.write("work_dir=$(pwd)\n\n")
                f.write("# crc32 perl script has a bug that reads the first hexadecimal in the path\n")
                f.write("# to prevent this from being an issue we change directory and pass filenames only\n")
                f.write('cd "$folder_path"\n\n')
                f.write('echo "name,csum" > "$csv_file"\n\n')
                f.write("# use extended globbing for pattern matching of both .txt and .tar.gz\n")
                f.write("shopt -s extglob\n\n")
                f.write("for file_path in ./@(*.txt|*.tar.gz); do\n\n")
                f.write('    checksum=$(crc32 "$file_path")\n')
                f.write("    checksum=$(echo \"$checksum\" | tr '[:lower:]' '[:upper:]')\n\n")
                f.write('    file_name=$(basename "$file_path")\n')
                f.write('    echo "$file_name,$checksum" >> "$csv_file"\n')
                f.write("done\n\n")
                f.write("# change back to working directory\n")
                f.write('cd "$work_dir"\n')
                f.write('echo "Checksums written to $csv_file"\n\n')

                # get checksums from archive
                f.write("\n## Compute checksums on data archive:\n\n")
                unsw_rds_checksum_path = archive_path.parent / "checksum_Archive_UNSW_RDS.csv"
                f.write(
                    f"java -Dmf.cfg=$CONFIG_FILE -jar /apps/unswdataarchive/2021-02-17/aterm.jar nogui \"asset.query :action get-values :where namespace>='/UNSW_RDS/{project_name}/{first_name}/{archive_relative_path}' :xpath -ename name name :xpath -ename csum content/csum :output-format csv :out file:{unsw_rds_checksum_path} :size 1000\"\n"
                )

                # compare checksums
                f.write("\n## Compare local and remote checksums:\n\n")
                f.write(
                    f"diff <(sort {str(local_checksum_path)}) <(sort {str(unsw_rds_checksum_path)}) > {str(diff_checksum_path)}\n"
                )

            f.write("\n## Remove local compressed data:\n\n")
            for batch_file in archive_batch_files:
                f.write(f"rm -v {str(Path(batch_file).with_suffix('.tar.gz'))} \n")

    except OSError:
        if Path(script_path).parent.exists():
            raise OSError(f"Could not write to {script_path}.")
        else:
            raise OSError(f"Could not write to {script_path}. Parent folder {Path(script_path).parent} does not exist.")


def archive(
    in_path: Union[Path, str], jobscript_path: Union[Path, str], first_name: str, project_name: str, input_type: str
) -> None:
    """
    Archive image files or directories using the specified input type.

    Parameters
    ----------
    in_path
        The path to the input image files or directories.
    jobscript_path
        The path to the job directory where the archiving script will be written.
    input_type
        The type of input, either "nd2" or "operetta".

    Returns
    -------
    None

    Raises
    ------
    ValueError
        If input_type is not "nd2" or "operetta".
    """

    check_config_file()

    today = datetime.today().strftime("%Y-%m-%d")

    in_path = Path(in_path)
    jobscript_path = Path(jobscript_path) / ("archive_data_blimp_" + today + ".sh")

    if input_type == "nd2":
        logger.debug("Setting up nd2 file archive")
        # Archive nd2 files individually since
        # these tend to be few in number
        image_files = find_nd2_files(in_path)
        write_archiving_script_nd2(image_files, jobscript_path, first_name, project_name)

    elif input_type == "operetta":
        logger.debug("Setting up operetta file archive")
        # Archive 'Images' directories after compressing into batches.
        # Also include associated metadata folders
        image_dirs = find_images_dirs(in_path)
        for i, image_dir in enumerate(image_dirs):
            # create archive directory if it does not exist
            archive_dir = Path(image_dir).parent / "Archive"
            if not archive_dir.exists():
                logger.debug(f"Archive directory does not exist. Creating {archive_dir}")
                archive_dir.mkdir(parents=True, exist_ok=True)
            else:
                logger.debug(f"Found existing archive directory at {archive_dir}")
            # split file list into batches and write as text files
            batches = split_operetta_files_into_archiving_batches(image_dir)
            write_archiving_batch_files(archive_dir=archive_dir, images_dir=image_dir, groups=batches)

            # setup the final path list to write in the archiving script
            sub_dirs = ["Archive", "Assaylayout", "FFC_Profile"]
            archive_dirs = [Path(image_dir).parent / s for s in sub_dirs]
            for d in archive_dirs:
                logger.debug(f"Creating archiving script for {d}")
            if i == 0:
                logger.debug(f"Creating new archiving script in {jobscript_path}")
                write_archiving_script_operetta(archive_dirs, jobscript_path, first_name, project_name, append=False)
            else:
                logger.debug(f"Appending to existing archving script in {jobscript_path}")
                write_archiving_script_operetta(archive_dirs, jobscript_path, first_name, project_name, append=True)

    else:
        logger.error(f"input_type {input_type} not recognised. Please specify 'nd2' or 'operetta' input_type.")
        raise ValueError("Input type not recognised")

    os.chmod(jobscript_path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IROTH)

    return
